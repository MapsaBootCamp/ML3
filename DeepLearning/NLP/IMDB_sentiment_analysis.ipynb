{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkTIm057HaIj",
        "outputId": "4a339ceb-662b-426b-f595-d7f96d7abdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  6 15:11:04 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "__T34uUXEPIR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9-k0vbdDG9a",
        "outputId": "54524463-9b54-412b-c857-07e8d0b20d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9222k      0  0:00:08  0:00:08 --:--:-- 16.2M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f21333abDc1k",
        "outputId": "525155b0-ebc7-4d53-86de-eb0a547f5676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname, val_dir / category / fname)"
      ],
      "metadata": {
        "id": "uiXjXEsGFZb2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory( \"aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcOHbxK5Dy78",
        "outputId": "645b68d5-dd99-48f8-edc4-1258def613a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing our datasets with a TextVectorization layer**"
      ],
      "metadata": {
        "id": "e1YIO7taHyin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(max_tokens=20000, output_mode=\"multi_hot\")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "RXGc_DLfGmlT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "9hrnoD3ZIiaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "hbdTbkVtOPZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",save_best_only=True)]\n",
        "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(),epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "8SniBjcdOzJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuring the TextVectorization layer to return bigrams**"
      ],
      "metadata": {
        "id": "cLdCzBlwRrTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(ngrams=2,max_tokens=20000,output_mode=\"multi_hot\")\n",
        "\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "HMIhlRcoRpzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",save_best_only=True)]\n",
        "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "ZurHlFydRxAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIGRAMS WITH TF-IDF ENCODING**"
      ],
      "metadata": {
        "id": "-qy23yFFWtqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# def tfidf(term, document, dataset):\n",
        "#   term_freq = document.count(term)\n",
        "#   doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
        "#   return term_freq / doc_freq"
      ],
      "metadata": {
        "id": "Tdr3I_XoWlVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"tf_idf\")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "ezPw4DUNW1P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",save_best_only=True)]\n",
        "model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "TKtByhF6XEE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing words as a sequence: The sequence model approach**"
      ],
      "metadata": {
        "id": "4418MXLqhMAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(max_tokens=max_tokens, output_mode=\"int\",output_sequence_length=max_length)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "w7UNPi-VX7TX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\",\n",
        "metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "-NtidbDliHLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "      keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "          save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "VeO-DlhUtqA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER**"
      ],
      "metadata": {
        "id": "2avd9VnAxOB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "  loss=\"binary_crossentropy\",\n",
        "  metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFQBko7Bun7Q",
        "outputId": "af614050-14f1-4550-cbf6-85748e47794b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               73984     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 98s 141ms/step - loss: 0.5195 - accuracy: 0.7464 - val_loss: 0.3782 - val_accuracy: 0.8448\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.3646 - accuracy: 0.8619 - val_loss: 0.3894 - val_accuracy: 0.8244\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.2915 - accuracy: 0.8945 - val_loss: 0.3393 - val_accuracy: 0.8610\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2397 - accuracy: 0.9147 - val_loss: 0.3280 - val_accuracy: 0.8746\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.2133 - accuracy: 0.9290 - val_loss: 0.3292 - val_accuracy: 0.8734\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.1784 - accuracy: 0.9410 - val_loss: 0.3562 - val_accuracy: 0.8784\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 27s 44ms/step - loss: 0.1553 - accuracy: 0.9499 - val_loss: 0.3523 - val_accuracy: 0.8724\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.1352 - accuracy: 0.9567 - val_loss: 0.3717 - val_accuracy: 0.8718\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 28s 44ms/step - loss: 0.1187 - accuracy: 0.9637 - val_loss: 0.4024 - val_accuracy: 0.8706\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 27s 44ms/step - loss: 0.1013 - accuracy: 0.9697 - val_loss: 0.4654 - val_accuracy: 0.8702\n",
            "782/782 [==============================] - 15s 18ms/step - loss: 0.3425 - accuracy: 0.8662\n",
            "Test acc: 0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using an Embedding layer with masking enabled**"
      ],
      "metadata": {
        "id": "OnFGo-jUT2k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
        "# some_input = [\n",
        "#  [4, 3, 2, 1, 0, 0, 0],\n",
        "#  [5, 4, 3, 2, 1, 0, 0],\n",
        "#  [2, 1, 0, 0, 0, 0, 0]\n",
        "# ]\n",
        "# mask = embedding_layer.compute_mask(some_input)\n",
        "# mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmVgS0LmWAS2",
        "outputId": "f86eea9e-ed74-48cc-be05-658aff771db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
              "array([[ True,  True,  True,  True, False, False, False],\n",
              "       [ True,  True,  True,  True,  True, False, False],\n",
              "       [ True,  True, False, False, False, False, False]])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "      input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "        save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
        "            callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3-vXeTvRXYi",
        "outputId": "7f60f509-53b7-40df-b9b0-31cb96711d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 89s 129ms/step - loss: 0.4446 - accuracy: 0.7862 - val_loss: 0.3214 - val_accuracy: 0.8648\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 48s 76ms/step - loss: 0.2786 - accuracy: 0.8885 - val_loss: 0.3346 - val_accuracy: 0.8698\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.2123 - accuracy: 0.9180 - val_loss: 0.2878 - val_accuracy: 0.8856\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1615 - accuracy: 0.9413 - val_loss: 0.4047 - val_accuracy: 0.8524\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.1268 - accuracy: 0.9540 - val_loss: 0.3566 - val_accuracy: 0.8838\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0987 - accuracy: 0.9651 - val_loss: 0.3780 - val_accuracy: 0.8684\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.0745 - accuracy: 0.9752 - val_loss: 0.4031 - val_accuracy: 0.8710\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 32s 50ms/step - loss: 0.0565 - accuracy: 0.9825 - val_loss: 0.4719 - val_accuracy: 0.8544\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.0439 - accuracy: 0.9858 - val_loss: 0.5287 - val_accuracy: 0.8742\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 0.5183 - val_accuracy: 0.8764\n",
            "782/782 [==============================] - 17s 19ms/step - loss: 0.2985 - accuracy: 0.8785\n",
            "Test acc: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**USING PRETRAINEDWORD EMBEDDINGS**"
      ],
      "metadata": {
        "id": "RVSVGq-xX7NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9evFn7_XUvk",
        "outputId": "2f5e668b-769d-4ee4-ef66-b0938eed7f0f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-06 21:50:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-04-06 21:50:29--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-04-06 21:50:29--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 41s  \n",
            "\n",
            "2023-04-06 21:53:12 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdBBNiLQYZI_",
        "outputId": "10aba3a4-d294-4ae5-d471-0d80233f9ec2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index.get(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzzuhV42aZ7T",
        "outputId": "0f5fcd9f-6528-4d35-fa12-8033b1785ca9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.26688  ,  0.39632  ,  0.6169   , -0.77451  , -0.1039   ,\n",
              "        0.26697  ,  0.2788   ,  0.30992  ,  0.0054685, -0.085256 ,\n",
              "        0.73602  , -0.098432 ,  0.5479   , -0.030305 ,  0.33479  ,\n",
              "        0.14094  , -0.0070003,  0.32569  ,  0.22902  ,  0.46557  ,\n",
              "       -0.19531  ,  0.37491  , -0.7139   , -0.51775  ,  0.77039  ,\n",
              "        1.0881   , -0.66011  , -0.16234  ,  0.9119   ,  0.21046  ,\n",
              "        0.047494 ,  1.0019   ,  1.1133   ,  0.70094  , -0.08696  ,\n",
              "        0.47571  ,  0.1636   , -0.44469  ,  0.4469   , -0.93817  ,\n",
              "        0.013101 ,  0.085964 , -0.67456  ,  0.49662  , -0.037827 ,\n",
              "       -0.11038  , -0.28612  ,  0.074606 , -0.31527  , -0.093774 ,\n",
              "       -0.57069  ,  0.66865  ,  0.45307  , -0.34154  , -0.7166   ,\n",
              "       -0.75273  ,  0.075212 ,  0.57903  , -0.1191   , -0.11379  ,\n",
              "       -0.10026  ,  0.71341  , -1.1574   , -0.74026  ,  0.40452  ,\n",
              "        0.18023  ,  0.21449  ,  0.37638  ,  0.11239  , -0.53639  ,\n",
              "       -0.025092 ,  0.31886  , -0.25013  , -0.63283  , -0.011843 ,\n",
              "        1.377    ,  0.86013  ,  0.20476  , -0.36815  , -0.68874  ,\n",
              "        0.53512  , -0.46556  ,  0.27389  ,  0.4118   , -0.854    ,\n",
              "       -0.046288 ,  0.11304  , -0.27326  ,  0.15636  , -0.20334  ,\n",
              "        0.53586  ,  0.59784  ,  0.60469  ,  0.13735  ,  0.42232  ,\n",
              "       -0.61279  , -0.38486  ,  0.35842  , -0.48464  ,  0.30728  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "UVRwR_DiY7Xw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "      max_tokens,\n",
        "      embedding_dim,\n",
        "      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "      trainable=False,\n",
        "      mask_zero=True,\n",
        ")"
      ],
      "metadata": {
        "id": "5DQcwerciVRX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"]\n",
        ")\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "           save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
        "          callbacks=callbacks\n",
        ")\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VCKrwwij49r",
        "outputId": "ceed9fc2-ad24-4042-fde9-f061be5cdb39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               34048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 47s 54ms/step - loss: 0.5653 - accuracy: 0.7050 - val_loss: 0.4923 - val_accuracy: 0.7682\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 0.4479 - accuracy: 0.7955 - val_loss: 0.4343 - val_accuracy: 0.8052\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.4012 - accuracy: 0.8192 - val_loss: 0.4046 - val_accuracy: 0.8292\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.3693 - accuracy: 0.8407 - val_loss: 0.3655 - val_accuracy: 0.8444\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 30s 48ms/step - loss: 0.3430 - accuracy: 0.8558 - val_loss: 0.3521 - val_accuracy: 0.8532\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 28s 44ms/step - loss: 0.3182 - accuracy: 0.8656 - val_loss: 0.4008 - val_accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 30s 47ms/step - loss: 0.3027 - accuracy: 0.8744 - val_loss: 0.3120 - val_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 27s 44ms/step - loss: 0.2879 - accuracy: 0.8827 - val_loss: 0.3311 - val_accuracy: 0.8570\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.2699 - accuracy: 0.8899 - val_loss: 0.3015 - val_accuracy: 0.8732\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 27s 43ms/step - loss: 0.2561 - accuracy: 0.8961 - val_loss: 0.3244 - val_accuracy: 0.8690\n",
            "782/782 [==============================] - 17s 19ms/step - loss: 0.2974 - accuracy: 0.8758\n",
            "Test acc: 0.876\n"
          ]
        }
      ]
    }
  ]
}